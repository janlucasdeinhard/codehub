{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random as rnd\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings,os,time,datetime,sys\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,input_shape,output_shape):\n",
    "        super(Net,self).__init__()\n",
    "        self.l_in = torch.nn.Linear(input_shape,16)\n",
    "        self.l0 = torch.nn.Linear(16,8)\n",
    "        self.l_out = torch.nn.Linear(8,output_shape)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=0.05)\n",
    "    def forward(self,x):\n",
    "        x = torch.nn.functional.relu(self.l_in(x))\n",
    "        x = torch.nn.functional.relu(self.l0(x))\n",
    "        x = torch.nn.functional.softmax(self.l_out(x))\n",
    "        return x\n",
    "    def discount_rewards(self,rewards,gamma=0.99):\n",
    "        r = np.array([gamma**k*rewards[k] for k in range(len(rewards))])\n",
    "        r = r[::-1].cumsum()[::-1]\n",
    "        return r-r.mean()\n",
    "    def action_probs_as_numpy(self,s):\n",
    "        action_p = self(torch.FloatTensor(s)).detach().numpy()\n",
    "        return action_p\n",
    "    def train(self,states,rewards,actions):\n",
    "        self.optimizer.zero_grad()\n",
    "        state_t = torch.FloatTensor(states)\n",
    "        reward_t = torch.FloatTensor(rewards)\n",
    "        action_t = torch.LongTensor(actions)\n",
    "        logprob = torch.log(net(state_t))\n",
    "        selected_logprobs = reward_t*torch.gather(logprob,1,action_t.reshape(action_t.shape[0],1)).squeeze()\n",
    "        loss = -selected_logprobs.mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 500\n",
    "BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "25/500 -> Avg rewards: -290.44587135423734\n",
      "50/500 -> Avg rewards: -285.979468977219\n",
      "75/500 -> Avg rewards: -285.92203946718257\n",
      "100/500 -> Avg rewards: -286.033462418021\n",
      "125/500 -> Avg rewards: -285.99775238216466\n",
      "150/500 -> Avg rewards: -285.95232146930397\n",
      "175/500 -> Avg rewards: -286.0035075297968\n",
      "200/500 -> Avg rewards: -285.9481456513953\n",
      "225/500 -> Avg rewards: -286.01512232418315\n",
      "250/500 -> Avg rewards: -285.96784960743685\n",
      "275/500 -> Avg rewards: -286.02444232330237\n",
      "300/500 -> Avg rewards: -285.98639611725105\n",
      "325/500 -> Avg rewards: -285.9247707969297\n",
      "350/500 -> Avg rewards: -285.9277821539909\n",
      "375/500 -> Avg rewards: -285.9086911858145\n",
      "400/500 -> Avg rewards: -285.96288333758434\n",
      "425/500 -> Avg rewards: -285.9602195138696\n",
      "450/500 -> Avg rewards: -285.931076544184\n",
      "475/500 -> Avg rewards: -285.96052784724947\n",
      "500/500 -> Avg rewards: -285.9900399811576\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "total_rewards = []\n",
    "batch_rewards = []\n",
    "batch_actions = []\n",
    "batch_states = []\n",
    "\n",
    "viz_step = int(5**np.floor(np.log10(NUM_EPISODES)))\n",
    "batch_counter = 1\n",
    "\n",
    "episodes = 0\n",
    "while episodes<NUM_EPISODES:\n",
    "\n",
    "    s0 = env.reset()\n",
    "\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        ap = net.action_probs_as_numpy(s0)\n",
    "        #if rnd.rand()<0.25: ap[1] = 0\n",
    "        #if ap.sum()==0: ap = np.ones(shape=[3])\n",
    "        action_probs = ap\n",
    "        action_c = rnd.choice(np.arange(env.action_space.n),p=action_probs)\n",
    "\n",
    "        s1,r,done,_ = env.step(action_c)\n",
    "        r = r + 0.5/(s1[0]-0.5)\n",
    "\n",
    "        states.append(s0)\n",
    "        rewards.append(r)\n",
    "        actions.append(action_c)\n",
    "\n",
    "        s0 = s1\n",
    "\n",
    "        if done:\n",
    "\n",
    "            total_rewards.append(sum(rewards))\n",
    "\n",
    "            batch_states.extend(states)\n",
    "            batch_rewards.extend(net.discount_rewards(rewards,gamma=1.2))\n",
    "            batch_actions.extend(actions)\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter==BATCH_SIZE:\n",
    "                net.train(batch_states,batch_rewards,batch_actions)\n",
    "                batch_states = []\n",
    "                batch_rewards = []\n",
    "                batch_actions = []\n",
    "                batch_counter = 1\n",
    "            episodes += 1\n",
    "    if episodes%viz_step==0: print('{}/{} -> Avg rewards: {}'.format(episodes,NUM_EPISODES,np.mean(total_rewards[-viz_step:])))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "s0 = env.reset()\n",
    "\n",
    "states = []\n",
    "rewards = []\n",
    "actions = []\n",
    "actionprobs = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    ap = net.action_probs_as_numpy(s0)\n",
    "    action_probs = ap/ap.sum()\n",
    "    action_c = rnd.choice(np.arange(env.action_space.n),p=action_probs)\n",
    "\n",
    "    s1,r,done,_ = env.step(action_c)\n",
    "    env.render()\n",
    "\n",
    "    states.append(s0)\n",
    "    rewards.append(r)\n",
    "    actions.append(action_c)\n",
    "    actionprobs.append(action_probs)\n",
    "\n",
    "    s0 = s1\n",
    "\n",
    "    if done: \n",
    "        net.train(states,rewards,actions)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
